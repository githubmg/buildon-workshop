{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install awswrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload data to S3\n",
    "\n",
    "First you need to create a bucket for this experiment. Upload the data from the following public location to your own S3 bucket.\n",
    "\n",
    "You can create a bucket from the following link: <a href='https://s3.console.aws.amazon.com/s3/home?region=us-east-1'> s3 console </a>\n",
    "\n",
    "To facilitate the work of the crawler we will use two different prefixes (folders): one for the billing information and one for reseller. \n",
    "\n",
    "\n",
    "\n",
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your bucket name\n",
    "your_bucket = 'XXXXXXXXX'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ml-lab-mggaska.s3.amazonaws.com/billing_sm.csv\n",
    "!wget https://ml-lab-mggaska.s3.amazonaws.com/reseller_sm.csv\n",
    "!wget https://ml-lab-mggaska.s3.amazonaws.com/awswrangler-1.9.6-py3.6.egg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3, os\n",
    "import awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(your_bucket).Object(os.path.join('billing', 'billing_sm.csv')).upload_file('billing_sm.csv')\n",
    "boto3.Session().resource('s3').Bucket(your_bucket).Object(os.path.join('reseller', 'reseller_sm.csv')).upload_file('reseller_sm.csv')\n",
    "boto3.Session().resource('s3').Bucket(your_bucket).Object(os.path.join('python', 'awswrangler-1.9.6-py3.6.egg')).upload_file('awswrangler-1.9.6-py3.6.egg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add athena full access permissions to SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the <a href='https://console.aws.amazon.com/iam/home?region=us-east-1#/roles'>IAM roles console</a> and attach the Amazon Athena full access policy to this role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Crawler\n",
    "\n",
    "To use this csv information in the context of a Glue ETL, first we have to create a Glue crawler pointing to the location of each file. The crawler will try to figure out the data types of each column. \n",
    "\n",
    "\n",
    "1. On the <a href='https://console.aws.amazon.com/iam/home?region=us-east-1#/roles'>IAM roles console</a> create an IAM role GlueCrawlerRole with the policy AWSGlueServiceRole and S3FullAccess.\n",
    "\n",
    "2. Go to  <a href='https://console.aws.amazon.com/glue/home?region=us-east-1#catalog:tab=crawlers'>Glue crawlers console</a> \n",
    "\n",
    "3. Add a Crawler : create one pointing to different each S3 locations (one to billing and one to reseller)\n",
    "\n",
    "    3.1 Fill  a Crawler Name: point a Data Store to specific S3 path, Navigate to your bucket and your folder: /billing, click \"Next\"\n",
    "    \n",
    "    3.2 Specify \"Yes\" to add a new Data Store and navigate to your bucket and your folder: /reseller, Click \"Next\" and select \"No\" when asking for add more Data stores, use an existing IAM role \"AWSGlueServiceRole\", add database \"implementationdb\", Click on \"Next\" and \"Finish\"\n",
    "    \n",
    "    3.3 After the crawler is created select \"Run it now\".\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configure Athena query destination\n",
    "\n",
    "Go to the <a href='https://console.aws.amazon.com/athena/home?force&region=us-east-1#query'>Athena console</a>.\n",
    "\n",
    "Under Settings in the top right corner set the query results location to s3://YOUR-BUCKET-NAME/athena-results/.\n",
    "\n",
    "To verify that your crawlers created correctly you can run the following query:\n",
    "    \n",
    "    select * from billing limit 3; \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Execute a query to create a sample View in Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "session = awswrangler.Session()\n",
    "query=('CREATE VIEW resellers_sample AS SELECT *'\n",
    "       'FROM billing where id_reseller '\n",
    "       'in (select distinct id_reseller from reseller TABLESAMPLE BERNOULLI(10))')\n",
    "\n",
    "df = session.pandas.read_sql_athena(\n",
    "    sql=query,\n",
    "    database=\"implementationdb\",\n",
    "    max_result_size=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
